{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0c376eb-b8e0-4d81-906a-89c8e9d49a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Document Ingestion Pipeline\n",
    "\n",
    "This notebook implements a complete document ingestion and chunking pipeline for PDF documents. The pipeline:\n",
    "\n",
    "1. **Loads PDF files** from a Unity Catalog Volume\n",
    "2. **Parses documents** using Databricks AI functions to extract structured content\n",
    "3. **Chunks content** into logical paragraphs with context (headers)\n",
    "4. **Prepares data** for downstream RAG (Retrieval-Augmented Generation) applications\n",
    "\n",
    "The final output is stored in the `docs_chunked` table with Change Data Feed enabled for incremental processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "295b3a60-9d4d-4b12-ba64-3a929f52f08b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Configure Source Volume\n",
    "\n",
    "Define the Unity Catalog Volume path where PDF documents are stored. This volume serves as the source for all documents to be ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b1eabff-8334-4537-9c74-4bc61c44e6fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "volume_path = \"/Volumes/mkr_gcp_sandbox_euw3/default/source_vol/docs/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "873c1331-44a9-49f0-882c-f7b874f91983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Parse PDF Documents\n",
    "\n",
    "Use the `ai_parse_document()` function to extract structured content from PDF files:\n",
    "\n",
    "* **Reads binary PDF files** from the volume using `binaryFile` format\n",
    "* **Extracts structured elements** including pages, text, tables, headers, and metadata\n",
    "* **Stores parsed results** in the `docs_parsed` table for inspection\n",
    "\n",
    "The parsed output includes:\n",
    "* `pages`: Page-level information\n",
    "* `elements`: Structured document elements (headers, paragraphs, tables)\n",
    "* `error_status`: Parsing error information (if any)\n",
    "* `metadata`: Document metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4116b7e1-6c40-4e76-8e83-8d788ab21e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (\n",
    "  spark.read.format(\"binaryFile\")\n",
    "    .load(volume_path+\"*.pdf\")\n",
    "    .withColumn(\"parsed\", ai_parse_document(col(\"content\")))\n",
    "    .select(\n",
    "      \"path\",\n",
    "      expr(\"parsed:document:pages\"),\n",
    "      expr(\"parsed:document:elements\"),\n",
    "      expr(\"parsed:error_status\"),\n",
    "      expr(\"parsed:metadata\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df.write.saveAsTable(\"docs_parsed\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaba3068-657d-4832-91a1-2a84755bfd08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Create Contextual Chunks\n",
    "\n",
    "Implement a custom chunking strategy that preserves document structure and context:\n",
    "\n",
    "### Chunking Logic\n",
    "The `extract_paragraph_texts` UDF processes document elements and creates chunks that include:\n",
    "* **Page headers**: Top-level context for each page\n",
    "* **Section headers**: Hierarchical section information\n",
    "* **Content**: Text and table content grouped into logical paragraphs\n",
    "\n",
    "### Strategy\n",
    "* Chunks are created when encountering new headers or at document boundaries\n",
    "* Each chunk includes the relevant page header and section header for context\n",
    "* Only `text` and `table` elements are included in the content\n",
    "* This approach ensures each chunk is self-contained with sufficient context for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40c7059-22ba-43aa-8bec-670f19922671",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def extract_paragraph_texts(elements):\n",
    "    allowed_types = {'text','table'}\n",
    "    results = []\n",
    "    current_page_header = \"\"\n",
    "    current_section_header = \"\"\n",
    "    current_paragraph = \"\"\n",
    "\n",
    "    if not elements:\n",
    "        return []\n",
    "    \n",
    "    for e in elements:\n",
    "        e = e.toPython()\n",
    "        e_type = e.get('type')\n",
    "        match e_type:\n",
    "            case 'page_header' | 'section_header':\n",
    "                if current_paragraph != \"\":\n",
    "                    chunk = current_page_header + \"\\n\\n\" + current_section_header + \"\\n\\n\" + current_paragraph\n",
    "                    results.append(chunk)\n",
    "                if e_type == 'page_header':\n",
    "                    current_page_header = e.get('content')\n",
    "                    current_section_header = \"\"\n",
    "                    current_paragraph = \"\"\n",
    "                elif e_type == 'section_header':\n",
    "                    current_section_header += \"\\n\\n\" + e.get('content')\n",
    "                    current_paragraph = \"\"\n",
    "            case 'table' | 'text':\n",
    "                current_paragraph += \"\\n\\n\" + e.get('content')\n",
    "            case _:\n",
    "                pass\n",
    "    if current_paragraph != \"\":\n",
    "        chunk = current_page_header + \"\\n\\n\" + current_section_header + \"\\n\\n\" + current_paragraph\n",
    "        results.append(chunk)\n",
    "    return results\n",
    "\n",
    "df_with_paragraphs = df.withColumn(\"paragraph_texts\", extract_paragraph_texts(expr(\"cast(elements as array<VARIANT>)\")))\n",
    "display(df_with_paragraphs.select(\"path\", \"paragraph_texts\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b81457d4-97de-4427-abec-7f75e2a32447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Create Final Chunked Table\n",
    "\n",
    "Explode the array of chunks into individual rows and create the final `docs_chunked` table:\n",
    "\n",
    "### Table Schema\n",
    "* `id`: Unique identifier for each chunk (monotonically increasing)\n",
    "* `path`: Source document path in the volume\n",
    "* `paragraph_text`: The chunk content with headers and context\n",
    "\n",
    "This table structure is optimized for:\n",
    "* Vector embedding generation\n",
    "* Semantic search and retrieval\n",
    "* RAG (Retrieval-Augmented Generation) applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad3e952-a5da-49af-abd8-0ee13b8b113b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, monotonically_increasing_id\n",
    "\n",
    "exploded_df = (\n",
    "    df_with_paragraphs\n",
    "    .withColumn(\"paragraph_text\", explode(\"paragraph_texts\"))\n",
    "    .withColumn(\"id\", monotonically_increasing_id())\n",
    "    .select(\"id\", \"path\", \"paragraph_text\")\n",
    ")\n",
    "\n",
    "exploded_df.write.saveAsTable(\"docs_chunked\")\n",
    "\n",
    "display(exploded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "decb4dd1-80b6-4cb9-8029-b468c7756656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Enable Change Data Feed\n",
    "\n",
    "Enable Change Data Feed (CDF) on the `docs_chunked` table to support:\n",
    "\n",
    "* **Incremental processing**: Track inserts, updates, and deletes\n",
    "* **Downstream pipelines**: Enable efficient incremental updates to vector indexes\n",
    "* **Audit trail**: Maintain history of changes to the chunked documents\n",
    "\n",
    "With CDF enabled, downstream systems can process only the changed data rather than reprocessing the entire table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bdef4a3-6490-42f7-9c9d-cdf61956efc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE docs_chunked\n",
    "  SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6279546542199085,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "doc_ingestor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
