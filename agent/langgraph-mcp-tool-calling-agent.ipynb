{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2026dfc8-763d-486a-a098-54cffed60919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks: Author and deploy an MCP tool-calling LangGraph agent\n",
    "\n",
    "This notebook shows how to author a LangGraph agent that connects to MCP servers hosted on Databricks. LangGraph's graph-based architecture gives you complete control over agent behavior, making it the right choice when you need custom workflows or multi-step reasoning patterns.\n",
    "\n",
    "Connect your agent to data and tools through MCP servers. Databricks provides managed MCP servers for Unity Catalog functions, vector search, and Genie spaces. You can also connect to custom MCP servers that you host as Databricks Apps. See [MCP on Databricks](https://docs.databricks.com/aws/en/generative-ai/mcp/).\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "- Author a LangGraph agent\n",
    "- Connect the agent to MCP servers to access Databricks-hosted tools\n",
    "- Test the agent and evaluate its responses using MLflow Evaluation\n",
    "- Log the agent with MLflow and deploy it to a model serving endpoint\n",
    "\n",
    "This notebook uses the  [`ResponsesAgent`](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ResponsesAgent) for Databrick compatibility.\n",
    "\n",
    "To learn more about authoring an agent using Mosaic AI Agent Framework, see Databricks documentation ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/author-agent) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/create-chat-model)).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Address all `TODO`s in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c107c2c4-df71-4402-b82f-82e2f2d64399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq --force-reinstall databricks-langchain databricks-agents uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c715704-ad67-45c9-a870-65728a01f38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18db682c-b2a3-48b0-9e91-b2b811419531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the agent code\n",
    "\n",
    "Define the agent code in a single cell below. This lets you easily write the agent\n",
    "code to a local Python file, using the `%%writefile` magic command, for subsequent\n",
    "logging and deployment.\n",
    "\n",
    "**What this code does at a high level:**\n",
    "\n",
    "1. **Connect to MCP servers using adapters**\n",
    "    The `DatabricksMCPServer` and `DatabricksMultiServerMCPClient` from `databricks_langchain` handle:\n",
    "    - Connections to Databricks MCP servers\n",
    "    - Authentication\n",
    "    - Automatic tool discovery and conversion to LangChain-compatible format\n",
    "\n",
    "2. **Build a LangGraph agent workflow using LangGraph `StateGraph`**\n",
    "\n",
    "3. **Handle streaming responses**\n",
    "    The `MCPToolCallingAgent` class wraps the LangGraph workflow to:\n",
    "    - Process streaming events from the agent graph in real-time\n",
    "    - Convert LangChain message formats to Mosaic AI-compatible format\n",
    "    - Enable MLflow tracing for each step of the agent workflow\n",
    "\n",
    "4. **Wrap with ResponsesAgent**\n",
    "    The agent is wrapped using `ResponsesAgent` for compatibility with Databricks\n",
    "    features like evaluation, deployment, and feedback collection.\n",
    "\n",
    "5. **MLflow autotracing**\n",
    "    Enable MLflow autologging to automatically trace LLM calls, tool invocations,\n",
    "    and agent state transitions.\n",
    "\n",
    "#### Agent tools\n",
    "\n",
    "This example connects to the Unity Catalog functions MCP server to access\n",
    "`system.ai.python_exec` (a built-in Python code interpreter). The code also\n",
    "includes commented-out examples for connecting to:\n",
    "- Custom MCP servers (hosted as Databricks Apps)\n",
    "- Vector search MCP servers (for semantic search over your data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a440f1ad-f51c-4f69-a081-94d8b2fa722d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent_mcp.py\n",
    "\n",
    "import asyncio\n",
    "from typing import Annotated, Any, AsyncGenerator, Generator, Optional, Sequence, TypedDict, Union\n",
    "\n",
    "import mlflow\n",
    "import nest_asyncio\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    DatabricksMCPServer,\n",
    "    DatabricksMultiServerMCPClient,\n",
    ")\n",
    "from langchain.messages import AIMessage, AIMessageChunk, AnyMessage\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "import json\n",
    "\n",
    "nest_asyncio.apply()\n",
    "############################################\n",
    "## Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-sonnet-4-5\"\n",
    "GENIE_SPACE_ID = \"01f101086a1711319ead12a273bb07f9\"\n",
    "VECTOR_SEARCH_SCHEMA = \"mkr_gcp_sandbox_euw3/default\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for our sports data customers.\n",
    "Use the vector search, which containd product documentation, to answer questions our product.\n",
    "Use the Genie to answer data related questions.\n",
    "If you don't know the answer, say 'I don't know', don't make up answers.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "host = workspace_client.config.host\n",
    "\n",
    "\n",
    "## There are three connection types:\n",
    "## 1. Managed MCP servers — fully managed by Databricks\n",
    "## 2. External MCP servers — hosted outside Databricks but proxied through a\n",
    "##    Managed MCP server proxy\n",
    "## 3. Custom MCP servers — MCP servers hosted as Databricks Apps\n",
    "##\n",
    "###############################################################################\n",
    "\n",
    "def create_tools():\n",
    "    # Import OBO credentials inside the function to ensure it's available at runtime\n",
    "    try:\n",
    "        from databricks_ai_bridge import ModelServingUserCredentials\n",
    "        obo_workspace_client = WorkspaceClient(credentials_strategy=ModelServingUserCredentials())\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create OBO workspace client: {e}\")\n",
    "        # Fallback to default workspace client if OBO fails\n",
    "        obo_workspace_client = workspace_client\n",
    "\n",
    "    databricks_mcp_client = DatabricksMultiServerMCPClient(\n",
    "        [\n",
    "            DatabricksMCPServer(\n",
    "                name=\"obo_vs_client\",\n",
    "                url=f\"{host}/api/2.0/mcp/vector-search/{VECTOR_SEARCH_SCHEMA}\",\n",
    "                workspace_client=obo_workspace_client\n",
    "            ),\n",
    "            DatabricksMCPServer(\n",
    "                name=\"obo_genie_client\",\n",
    "                url=f\"{host}/api/2.0/mcp/genie/{GENIE_SPACE_ID}\",\n",
    "                workspace_client=obo_workspace_client\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return databricks_mcp_client.get_tools()\n",
    "\n",
    "\n",
    "\n",
    "# The state for the agent workflow, including the conversation and any custom data\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    tools = asyncio.run(create_tools())\n",
    "    model = model.bind_tools(tools)  # Bind tools to the model\n",
    "\n",
    "    # Function to check if agent should continue or finish based on last message\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If function (tool) calls are present, continue; otherwise, end\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    # Preprocess: optionally prepend a system prompt to the conversation history\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    model_runnable = preprocessor | model  # Chain the preprocessor and the model\n",
    "\n",
    "    # The function to invoke the model within the workflow\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)  # Create the agent's state machine\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))  # Agent node (LLM)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))  # Tools node\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")  # Start at agent node\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",  # If the model requests a tool call, move to tools node\n",
    "            \"end\": END,  # Otherwise, end the workflow\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")  # After tools are called, return to agent node\n",
    "\n",
    "    # Compile and return the tool-calling agent workflow\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# ResponsesAgent class to wrap the compiled agent and make it compatible with Mosaic AI Responses API\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    # def __init__(self, agent):\n",
    "    #     self.agent = agent\n",
    "\n",
    "    # Make a prediction (single-step) for the agent\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\" or event.type == \"error\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    async def _predict_stream_async(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> AsyncGenerator[ResponsesAgentStreamEvent, None]:\n",
    "        agent = create_tool_calling_agent(model=llm, system_prompt=system_prompt)\n",
    "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        # Stream events from the agent graph\n",
    "        async for event in agent.astream(\n",
    "            {\"messages\": cc_msgs}, stream_mode=[\"updates\", \"messages\"]\n",
    "        ):\n",
    "            if event[0] == \"updates\":\n",
    "                # Stream updated messages from the workflow nodes\n",
    "                for node_data in event[1].values():\n",
    "                    if len(node_data.get(\"messages\", [])) > 0:\n",
    "                        all_messages = []\n",
    "                        for msg in node_data[\"messages\"]:\n",
    "                            if isinstance(msg, ToolMessage) and not isinstance(msg.content, str):\n",
    "                                msg.content = json.dumps(msg.content)\n",
    "                            all_messages.append(msg)\n",
    "                        for item in output_to_responses_items_stream(all_messages):\n",
    "                            yield item\n",
    "            elif event[0] == \"messages\":\n",
    "                # Stream generated text message chunks\n",
    "                try:\n",
    "                    chunk = event[1][0]\n",
    "                    if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                        yield ResponsesAgentStreamEvent(\n",
    "                            **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                        )\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # Stream predictions for the agent, yielding output as it's generated\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        agen = self._predict_stream_async(request)\n",
    "\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "\n",
    "        ait = agen.__aiter__()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                item = loop.run_until_complete(ait.__anext__())\n",
    "            except StopAsyncIteration:\n",
    "                break\n",
    "            else:\n",
    "                yield item\n",
    "\n",
    "\n",
    "# Initialize the entire agent, including MCP tools and workflow\n",
    "def initialize_agent():\n",
    "    return LangGraphResponsesAgent()\n",
    "\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = initialize_agent()\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e95c42b9-7cd5-4a76-a5c2-cf6ed7af87e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output and tool-calling abilities. Since this notebook called `mlflow.langchain.autolog()`, you can view the trace for each step the agent takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "076c0e67-b4ab-45b0-bb13-5b7952402270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6da1040-45b9-4e8e-9640-e2b64931c1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent_mcp import AGENT\n",
    "\n",
    "AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"What is can I do with fixtures in the product?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10003231-75e0-4f15-b453-34d16e46077e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for chunk in AGENT.predict_stream(\n",
    "    {\"input\": [{\"role\": \"user\", \"content\": \"What is can I do with fixtures in the product?\"}]}\n",
    "):\n",
    "    print(chunk, \"-----------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce59709-8c40-49f0-8e04-f8307b5ac1cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent as an MLflow model\n",
    "\n",
    "Log the agent as code from the `agent_mcp.py` file. See [Deploy an agent that connects to Databricks MCP servers](https://docs.databricks.com/aws/en/generative-ai/mcp/managed-mcp#deploy-your-agent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c71f03ca-930f-4a0f-8bca-b50c1a2240d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.auth_policy import AuthPolicy, SystemAuthPolicy, UserAuthPolicy\n",
    "from mlflow.models.resources import DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-sonnet-4-5\"\n",
    "# System policy: resources accessed with system credentials\n",
    "system_policy = SystemAuthPolicy(\n",
    "    resources=[DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    ")\n",
    "\n",
    "# User policy: API scopes for OBO access\n",
    "user_policy = UserAuthPolicy(api_scopes=[\n",
    "    \"serving.serving-endpoints\",\n",
    "    \"mcp.vectorsearch\",\n",
    "    \"mcp.genie\"\n",
    "])\n",
    "\n",
    "# Log the agent with both policies\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent_mcp.py\",\n",
    "        auth_policy=AuthPolicy(\n",
    "            system_auth_policy=system_policy,\n",
    "            user_auth_policy=user_policy,\n",
    "        ),\n",
    "        pip_requirements=[\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"mcp=={get_distribution('mcp').version}\",\n",
    "            f\"databricks-mcp=={get_distribution('databricks-mcp').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdd669d-7ade-4ff1-a5d8-1f540948d57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with [Agent Evaluation](https://docs.databricks.com/mlflow3/genai/eval-monitor)\n",
    "\n",
    "You can edit the requests or expected responses in your evaluation dataset and run evaluation as you iterate your agent, leveraging mlflow to track the computed quality metrics.\n",
    "\n",
    "Evaluate your agent with one of our [predefined LLM scorers](https://docs.databricks.com/mlflow3/genai/eval-monitor/predefined-judge-scorers), or try adding [custom metrics](https://docs.databricks.com/mlflow3/genai/eval-monitor/custom-scorers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0266ee29-1fc8-41cc-84c0-63ffcef9f392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import RelevanceToQuery, Safety, RetrievalRelevance, RetrievalGroundedness\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What is can I do with fixtures in the product?\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": \"Based on the product documentation, here's what you can do with fixtures in the product:\\n\\n## Fixture Ordering\\n- **Order fixtures** for InPlay or PreMatch events across different hierarchy levels (sport, location, competition, and individual fixtures)\\n- **View fixtures** with various statuses:\\n  - **InPlay**: NSY, About to Start, In Progress, Lost Coverage, Interrupted (max 5 days from start date), Postponed\\n  - **PreMatch**: NSY, About to Start, Postponed\\n- **Configure market settings** with a hierarchy: Package configuration → Sport → Location → Competition → Fixture\\n\\n## Fixture Management\\n- **Remove orders/subscriptions** from sport/location/competition levels\\n  - When removing an ordered component, all related fixtures are automatically removed\\n  - For fixtures already in progress, all relevant markets are suspended and end-of-event messages are sent\\n\\n## Trading Floor Operations\\n- **Suspend/Unsuspend odds** at different levels:\\n  - **Suspend all** - suspend all fixture's markets\\n  - **Suspend market** - suspend only a specific market\\n  - **Suspend line** - suspend only a specific line\\n- **Manual suspension control** - easily suspend/unsuspend with visual indicators (red button and red frame around suspended markets)\\n\\n## Fixture Logs\\n- **View comprehensive logs** of all your ordered fixtures\\n- **Access fixture-specific logs** by selecting individual fixtures from the list\\n\\nThe system provides flexibility to manage fixtures at various levels of granularity, from broad sport-level ordering down to specific fixture and market control.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()], # add more scorers here if they're applicable\n",
    ")\n",
    "\n",
    "# Review the evaluation results in the MLfLow UI (see console output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ffc536-f9c1-45b0-890c-9b382c9552ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"What is can I do with fixtures in the product?\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3989e7dd-c714-4ea5-ac33-e50b6b8e22ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "\n",
    "Before you deploy the agent, you must register the agent to Unity Catalog.\n",
    "\n",
    "- **TODO** Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c22837d-2b8c-4f92-8653-135268632270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"mkr_gcp_sandbox_euw3\"\n",
    "schema = \"default\"\n",
    "model_name = \"agent_mcp\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23daef3-24d6-4708-a01d-5d42ca861384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95f3fb0-c5d3-473e-87c0-c34352fb81e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME, \n",
    "    uc_registered_model_info.version,\n",
    "    # ==============================================================================\n",
    "    # TODO: ONLY UNCOMMENT AND CONFIGURE THE ENVIRONMENT_VARS SECTION BELOW\n",
    "    #       IF YOU ARE USING OAUTH/SERVICE PRINCIPAL FOR CUSTOM MCP SERVERS.\n",
    "    #       For managed MCP (the default), LEAVE THIS SECTION COMMENTED OUT.\n",
    "    # ==============================================================================\n",
    "    # environment_vars={\n",
    "    #     \"DATABRICKS_CLIENT_ID\": DATABRICKS_CLIENT_ID,\n",
    "    #     \"DATABRICKS_CLIENT_SECRET\": f\"{{{{secrets/{client_secret_scope_name}/{client_secret_key_name}}}}}\"\n",
    "    # },\n",
    "    tags = {\"endpointSource\": \"docs\"},\n",
    "    deploy_feedback_model=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "langgraph-mcp-tool-calling-agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
